{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a>\n",
        "\n",
        "**[<font size=6>üèÉHAR</font>](https://www.kaggle.com/competitions/29jan24hse-har/rules)**. [**Instructions**](https://colab.research.google.com/drive/1owkYjuRGkx050LQnM3b3yTzd0Dr2XbeV) for running Colabs."
      ],
      "metadata": {
        "id": "H211VO45y3re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>**CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes."
      ],
      "metadata": {
        "id": "B9WbQS1vzgf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT3dcYSxxx_K"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZoMVT8krwWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle > log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                                           # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json > log                   # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                              # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v 29jan24hse-har           # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log                          # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                                        # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "!kaggle competitions leaderboard --show                       # print public leaderboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuKXMRjsyxoK",
        "outputId": "f3e2f5d3-4938-4d3a-a845-3fd997af5bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: 29jan24hse-har\n",
            "100% 601M/601M [00:21<00:00, 28.7MB/s]\n",
            "Using competition: 29jan24hse-har\n",
            "  teamId  teamName           submissionDate       score    \n",
            "--------  -----------------  -------------------  -------  \n",
            "11629292  Z                  2024-02-18 16:05:47  0.97284  \n",
            "11633526  Maxim Smeyanov     2024-02-18 01:36:03  0.96809  \n",
            "11633785  Zapriagaeva Vlada  2024-02-18 19:45:04  0.96741  \n",
            "11578655  AI                 2024-02-18 09:45:17  0.96673  \n",
            "11651220  AD                 2024-02-18 11:40:18  0.96401  \n",
            "11610716  R                  2024-02-18 18:19:04  0.96334  \n",
            "11633731  S                  2024-02-18 15:52:03  0.96334  \n",
            "11654783  Gennady Fomin      2024-02-18 20:24:55  0.96334  \n",
            "11588231  G                  2024-02-18 19:46:08  0.96130  \n",
            "11579168  A                  2024-02-18 18:26:44  0.95994  \n",
            "11634188  P                  2024-02-18 20:44:46  0.95858  \n",
            "11633671  AA                 2024-02-18 17:23:40  0.95790  \n",
            "11631970  AS                 2024-02-18 20:15:41  0.95790  \n",
            "11591535  Artem Ilin         2024-02-03 14:46:22  0.95723  \n",
            "11591404  AF                 2024-02-18 20:20:33  0.95655  \n",
            "11619534  AJ                 2024-02-12 15:15:19  0.95519  \n",
            "11633717  AE                 2024-02-12 18:13:35  0.95315  \n",
            "11634112  Q                  2024-02-18 20:08:10  0.95247  \n",
            "11626825  X                  2024-02-18 10:38:33  0.95179  \n",
            "11596138  W                  2024-02-08 17:39:51  0.95112  \n",
            "11591981  AR                 2024-02-17 18:06:22  0.94976  \n",
            "11651999  Pavel Nedbay       2024-02-18 18:42:56  0.94908  \n",
            "11633549  Ilshat Dineev      2024-02-18 19:57:39  0.94840  \n",
            "11633581  Naumenko Daria     2024-02-18 17:27:47  0.94772  \n",
            "11633038  AT                 2024-02-18 14:54:14  0.94704  \n",
            "11633425  O                  2024-02-12 16:51:20  0.94636  \n",
            "11595367  BAKUROV Timofei    2024-02-03 13:38:09  0.94433  \n",
            "11637803  Y                  2024-02-18 10:31:47  0.94433  \n",
            "11654661  Dmitrii Kim        2024-02-18 20:16:13  0.93754  \n",
            "11633569  IlyaKozhevnikov    2024-02-12 17:26:00  0.93686  \n",
            "11574845  Baseline           2024-01-29 19:58:15  0.92396  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import numpy as np, pandas as pd, time, os, random\n",
        "\n",
        "np.set_printoptions(linewidth=10000, precision=2, edgeitems=20, suppress=True)\n",
        "pd.set_option('display.max_colwidth', 1000, 'display.max_columns', 100, 'display.width', 1000, 'display.max_rows', 4)\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchsummary import summary\n",
        "\n",
        "# Fallback options for TensorFlow + Keras and for SKLearn\n",
        "# import tensorflow as tf, tensorflow.keras as keras\n",
        "# from sklearn.neural_network import MLPClassifier   # SKLearn's MLP is optimised for CPU (and doesn't use GPU)\n",
        "# from keras.layers import Flatten, Dense\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'‚è≥ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOhT2o18yxld",
        "outputId": "613c4a80-f02c-4a5c-88c8-015173a2dc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.02 s, sys: 414 ms, total: 2.43 s\n",
            "Wall time: 4.28 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Always seed your experiments\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    print(f\"Random seed set as {seed}\")\n",
        "\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "nELogoFQ5sRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b56203-126f-417e-8842-a9d0286d66e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set as 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if cuda activated.\n",
        "# If not, go to Runtime -> Change runtime type. Select 'T4 GPU'\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('cuda activated')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('cpu activated')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ7jNmp-LYNB",
        "outputId": "18962c1b-b917-4c5c-daab-388687c74e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda activated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "vX  = pd.read_csv('testX.csv', index_col='id')  # load testing input features X (only)\n",
        "%time\n",
        "tYX = pd.read_csv('trainYX.csv')                # partially load training labels Y and input features X\n",
        "tYX  # 561 input features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "7VA1oBPyyxii",
        "outputId": "e92978c7-f8d4-49de-aa8d-97f165481a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 ¬µs, sys: 0 ns, total: 3 ¬µs\n",
            "Wall time: 6.68 ¬µs\n",
            "CPU times: user 3 ¬µs, sys: 1 ¬µs, total: 4 ¬µs\n",
            "Wall time: 6.68 ¬µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        y       0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42      43     44      45      46      47      48  ...     511     512     513     514     515     516     517     518     519     520    521     522     523     524     525     526     527     528     529     530     531    532     533    534     535     536     537     538     539     540     541     542     543     544     545     546     547     548     549    550     551     552     553     554     555     556     557     558     559     560\n",
              "0       5  0.2778  0.0092 -0.0676 -0.9785 -0.9160 -0.9610 -0.9834 -0.9170 -0.9590 -0.9390 -0.4230 -0.7520  0.8496  0.6226  0.8400 -0.9434 -0.9614 -1.0370 -1.0150 -1.0070 -0.9640 -0.9550 -0.6772  0.0568  0.0192  0.5900 -0.3162  0.1833  0.4440 -0.2622  0.1092  0.4468 -0.4443 -0.1484  0.1718 -0.2727  0.0954 -0.4720 -0.5264  0.2332  0.9640 -0.1309  0.1071 -0.9814 -0.948 -0.9727 -0.9720 -0.9575 -0.9585  ... -0.9126 -0.2037 -0.5300 -0.8164 -0.9170 -0.8850 -0.9033 -0.9120 -0.9750 -0.9326 -1.014 -0.9560 -0.6780 -0.9966 -0.6180 -0.1021 -0.5977 -0.9546 -0.9110 -0.9260 -0.9297 -1.017 -0.9460 -1.022 -0.9570 -0.2930 -1.0100 -0.3455 -0.1411 -0.5215 -0.9585 -0.9160 -0.9434 -0.9414 -0.9750 -0.9414 -0.9890 -0.9610 -0.4453 -1.002 -0.5415 -0.0308 -0.5093  0.0380 -0.0912 -0.1415 -0.1316 -0.8200  0.1721 -0.0535\n",
              "1       1  0.2454  0.0073 -0.1046 -0.2010  0.1426 -0.2668 -0.2776  0.0648 -0.2605 -0.0572 -0.0364 -0.2830 -0.2830 -0.1448  0.4443 -0.0844 -0.6733 -0.7603 -0.7847 -0.4136 -0.3633 -0.1837  0.2830  0.5100  0.0582 -0.2502  0.3079 -0.1384  0.0822  0.0902 -0.0034  0.1969  0.0538  0.2996 -0.0258  0.0936 -0.3472 -0.1434 -0.4058  0.3690  0.9326 -0.2942 -0.0916 -0.9966 -0.964 -0.9663 -0.9746 -0.9736 -0.9634  ... -0.8115  0.4165 -0.4731 -0.8210  0.2542  0.2410  0.2688  0.0928 -0.7710  0.2430 -0.221 -0.1018  0.7134 -0.8994 -0.0642 -0.0842 -0.4750 -0.1345 -0.3853 -0.2573 -0.5430 -0.757 -0.1365 -0.677 -0.1826  0.6777 -0.7866  0.3240 -0.6206 -0.8530 -0.2500 -0.3025 -0.3176 -0.3198 -0.6426 -0.2488 -0.7236 -0.2512  0.6177 -0.910  0.1069 -0.0397 -0.4220  0.5480  0.6455  0.2296 -0.0335 -0.7000  0.2998  0.0880\n",
              "...    ..     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...\n",
              "499998  4  0.2740 -0.0132 -0.1257 -0.9834 -1.0020 -0.9590 -0.9897 -0.9746 -0.9873 -0.9346 -0.5630 -0.8394  0.8306  0.6846  0.8350 -0.9840 -0.9824 -0.9960 -1.0200 -0.9950 -1.0160 -0.9590 -0.6500 -0.5225 -0.7974  0.5020 -0.2532  0.3723  0.1772  0.2920 -0.2756  0.3179 -0.1398  0.0948  0.0180 -0.1853  0.1871  0.0790 -0.0402 -0.0880  0.9785 -0.0442 -0.0532 -0.9950 -1.028 -0.9790 -1.0010 -0.9697 -1.0060  ... -0.2356  0.4312 -0.6030 -0.8706 -0.9700 -0.9863 -1.0010 -0.9990 -0.9927 -0.9910 -1.020 -1.0210 -0.9920 -0.9736  0.3857 -0.4620 -0.7485 -0.9985 -0.9575 -0.9897 -0.9814 -1.000 -0.9990 -1.008 -0.9660 -0.8574 -0.9210  0.1049 -0.6284 -0.8970 -1.0200 -1.0150 -0.9750 -1.0170 -0.9746 -0.9937 -0.9927 -0.9950 -1.0030 -0.844  0.2454 -0.3782 -0.7183 -0.0227  0.1957  0.1864  0.4556 -0.9326  0.1137  0.0595\n",
              "499999  5  0.2695 -0.0251 -0.1010 -1.0170 -0.9050 -0.9375 -0.9736 -0.8920 -0.9673 -0.9575 -0.5293 -0.8022  0.8530  0.6714  0.8480 -0.9624 -1.0205 -0.9900 -0.9600 -0.9960 -0.9480 -0.9720 -0.7320 -0.5117 -0.3535  0.3710 -0.2270  0.2700 -0.0636 -0.2438  0.0608  0.2050 -0.0218 -0.1199  0.0678  0.0154 -0.1132 -0.2886 -0.3882  0.6284  0.9966 -0.1277  0.0722 -1.0050 -0.925 -0.9440 -1.0050 -0.9824 -0.9233  ... -0.9500  0.0488 -0.3591 -0.7050 -1.0240 -0.9790 -0.9746 -0.9814 -0.9920 -0.9814 -1.013 -0.9860 -0.9650 -1.0150 -0.1430 -0.1555 -0.5180 -0.9320 -0.9200 -0.9424 -0.9326 -0.932 -0.9170 -0.985 -0.9463 -0.4020 -0.9640 -0.3160 -0.0948 -0.4695 -0.9590 -0.9500 -0.9976 -0.9680 -1.0340 -0.9727 -0.9900 -0.9790 -0.6980 -1.017 -0.4863  0.0084 -0.3293 -0.0127 -0.1399  0.4624 -0.7610 -0.8696  0.1720 -0.0272\n",
              "\n",
              "[500000 rows x 562 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-260aeff3-5d3e-4432-8032-05ae93ea4164\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>...</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "      <th>520</th>\n",
              "      <th>521</th>\n",
              "      <th>522</th>\n",
              "      <th>523</th>\n",
              "      <th>524</th>\n",
              "      <th>525</th>\n",
              "      <th>526</th>\n",
              "      <th>527</th>\n",
              "      <th>528</th>\n",
              "      <th>529</th>\n",
              "      <th>530</th>\n",
              "      <th>531</th>\n",
              "      <th>532</th>\n",
              "      <th>533</th>\n",
              "      <th>534</th>\n",
              "      <th>535</th>\n",
              "      <th>536</th>\n",
              "      <th>537</th>\n",
              "      <th>538</th>\n",
              "      <th>539</th>\n",
              "      <th>540</th>\n",
              "      <th>541</th>\n",
              "      <th>542</th>\n",
              "      <th>543</th>\n",
              "      <th>544</th>\n",
              "      <th>545</th>\n",
              "      <th>546</th>\n",
              "      <th>547</th>\n",
              "      <th>548</th>\n",
              "      <th>549</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2778</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>-0.0676</td>\n",
              "      <td>-0.9785</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9390</td>\n",
              "      <td>-0.4230</td>\n",
              "      <td>-0.7520</td>\n",
              "      <td>0.8496</td>\n",
              "      <td>0.6226</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9614</td>\n",
              "      <td>-1.0370</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-1.0070</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.9550</td>\n",
              "      <td>-0.6772</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.5900</td>\n",
              "      <td>-0.3162</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.4440</td>\n",
              "      <td>-0.2622</td>\n",
              "      <td>0.1092</td>\n",
              "      <td>0.4468</td>\n",
              "      <td>-0.4443</td>\n",
              "      <td>-0.1484</td>\n",
              "      <td>0.1718</td>\n",
              "      <td>-0.2727</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>-0.4720</td>\n",
              "      <td>-0.5264</td>\n",
              "      <td>0.2332</td>\n",
              "      <td>0.9640</td>\n",
              "      <td>-0.1309</td>\n",
              "      <td>0.1071</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.948</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9126</td>\n",
              "      <td>-0.2037</td>\n",
              "      <td>-0.5300</td>\n",
              "      <td>-0.8164</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.8850</td>\n",
              "      <td>-0.9033</td>\n",
              "      <td>-0.9120</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-1.014</td>\n",
              "      <td>-0.9560</td>\n",
              "      <td>-0.6780</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.6180</td>\n",
              "      <td>-0.1021</td>\n",
              "      <td>-0.5977</td>\n",
              "      <td>-0.9546</td>\n",
              "      <td>-0.9110</td>\n",
              "      <td>-0.9260</td>\n",
              "      <td>-0.9297</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.9460</td>\n",
              "      <td>-1.022</td>\n",
              "      <td>-0.9570</td>\n",
              "      <td>-0.2930</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-0.3455</td>\n",
              "      <td>-0.1411</td>\n",
              "      <td>-0.5215</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9890</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.4453</td>\n",
              "      <td>-1.002</td>\n",
              "      <td>-0.5415</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.5093</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>-0.0912</td>\n",
              "      <td>-0.1415</td>\n",
              "      <td>-0.1316</td>\n",
              "      <td>-0.8200</td>\n",
              "      <td>0.1721</td>\n",
              "      <td>-0.0535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>-0.1046</td>\n",
              "      <td>-0.2010</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>-0.2668</td>\n",
              "      <td>-0.2776</td>\n",
              "      <td>0.0648</td>\n",
              "      <td>-0.2605</td>\n",
              "      <td>-0.0572</td>\n",
              "      <td>-0.0364</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.1448</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>-0.0844</td>\n",
              "      <td>-0.6733</td>\n",
              "      <td>-0.7603</td>\n",
              "      <td>-0.7847</td>\n",
              "      <td>-0.4136</td>\n",
              "      <td>-0.3633</td>\n",
              "      <td>-0.1837</td>\n",
              "      <td>0.2830</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>-0.2502</td>\n",
              "      <td>0.3079</td>\n",
              "      <td>-0.1384</td>\n",
              "      <td>0.0822</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.1969</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>-0.0258</td>\n",
              "      <td>0.0936</td>\n",
              "      <td>-0.3472</td>\n",
              "      <td>-0.1434</td>\n",
              "      <td>-0.4058</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.9326</td>\n",
              "      <td>-0.2942</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.964</td>\n",
              "      <td>-0.9663</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.9634</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.8115</td>\n",
              "      <td>0.4165</td>\n",
              "      <td>-0.4731</td>\n",
              "      <td>-0.8210</td>\n",
              "      <td>0.2542</td>\n",
              "      <td>0.2410</td>\n",
              "      <td>0.2688</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>-0.7710</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.1018</td>\n",
              "      <td>0.7134</td>\n",
              "      <td>-0.8994</td>\n",
              "      <td>-0.0642</td>\n",
              "      <td>-0.0842</td>\n",
              "      <td>-0.4750</td>\n",
              "      <td>-0.1345</td>\n",
              "      <td>-0.3853</td>\n",
              "      <td>-0.2573</td>\n",
              "      <td>-0.5430</td>\n",
              "      <td>-0.757</td>\n",
              "      <td>-0.1365</td>\n",
              "      <td>-0.677</td>\n",
              "      <td>-0.1826</td>\n",
              "      <td>0.6777</td>\n",
              "      <td>-0.7866</td>\n",
              "      <td>0.3240</td>\n",
              "      <td>-0.6206</td>\n",
              "      <td>-0.8530</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>-0.3025</td>\n",
              "      <td>-0.3176</td>\n",
              "      <td>-0.3198</td>\n",
              "      <td>-0.6426</td>\n",
              "      <td>-0.2488</td>\n",
              "      <td>-0.7236</td>\n",
              "      <td>-0.2512</td>\n",
              "      <td>0.6177</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>0.1069</td>\n",
              "      <td>-0.0397</td>\n",
              "      <td>-0.4220</td>\n",
              "      <td>0.5480</td>\n",
              "      <td>0.6455</td>\n",
              "      <td>0.2296</td>\n",
              "      <td>-0.0335</td>\n",
              "      <td>-0.7000</td>\n",
              "      <td>0.2998</td>\n",
              "      <td>0.0880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499998</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2740</td>\n",
              "      <td>-0.0132</td>\n",
              "      <td>-0.1257</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-1.0020</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9346</td>\n",
              "      <td>-0.5630</td>\n",
              "      <td>-0.8394</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>0.6846</td>\n",
              "      <td>0.8350</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0160</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.6500</td>\n",
              "      <td>-0.5225</td>\n",
              "      <td>-0.7974</td>\n",
              "      <td>0.5020</td>\n",
              "      <td>-0.2532</td>\n",
              "      <td>0.3723</td>\n",
              "      <td>0.1772</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>-0.2756</td>\n",
              "      <td>0.3179</td>\n",
              "      <td>-0.1398</td>\n",
              "      <td>0.0948</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>-0.1853</td>\n",
              "      <td>0.1871</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>-0.0402</td>\n",
              "      <td>-0.0880</td>\n",
              "      <td>0.9785</td>\n",
              "      <td>-0.0442</td>\n",
              "      <td>-0.0532</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.028</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9697</td>\n",
              "      <td>-1.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2356</td>\n",
              "      <td>0.4312</td>\n",
              "      <td>-0.6030</td>\n",
              "      <td>-0.8706</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>-0.9863</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9910</td>\n",
              "      <td>-1.020</td>\n",
              "      <td>-1.0210</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>-0.4620</td>\n",
              "      <td>-0.7485</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-1.008</td>\n",
              "      <td>-0.9660</td>\n",
              "      <td>-0.8574</td>\n",
              "      <td>-0.9210</td>\n",
              "      <td>0.1049</td>\n",
              "      <td>-0.6284</td>\n",
              "      <td>-0.8970</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9937</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0030</td>\n",
              "      <td>-0.844</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>-0.3782</td>\n",
              "      <td>-0.7183</td>\n",
              "      <td>-0.0227</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.1864</td>\n",
              "      <td>0.4556</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>0.1137</td>\n",
              "      <td>0.0595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499999</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2695</td>\n",
              "      <td>-0.0251</td>\n",
              "      <td>-0.1010</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9050</td>\n",
              "      <td>-0.9375</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.8920</td>\n",
              "      <td>-0.9673</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.5293</td>\n",
              "      <td>-0.8022</td>\n",
              "      <td>0.8530</td>\n",
              "      <td>0.6714</td>\n",
              "      <td>0.8480</td>\n",
              "      <td>-0.9624</td>\n",
              "      <td>-1.0205</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9600</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-0.9480</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.7320</td>\n",
              "      <td>-0.5117</td>\n",
              "      <td>-0.3535</td>\n",
              "      <td>0.3710</td>\n",
              "      <td>-0.2270</td>\n",
              "      <td>0.2700</td>\n",
              "      <td>-0.0636</td>\n",
              "      <td>-0.2438</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>-0.1132</td>\n",
              "      <td>-0.2886</td>\n",
              "      <td>-0.3882</td>\n",
              "      <td>0.6284</td>\n",
              "      <td>0.9966</td>\n",
              "      <td>-0.1277</td>\n",
              "      <td>0.0722</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.925</td>\n",
              "      <td>-0.9440</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9233</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>0.0488</td>\n",
              "      <td>-0.3591</td>\n",
              "      <td>-0.7050</td>\n",
              "      <td>-1.0240</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.013</td>\n",
              "      <td>-0.9860</td>\n",
              "      <td>-0.9650</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.1430</td>\n",
              "      <td>-0.1555</td>\n",
              "      <td>-0.5180</td>\n",
              "      <td>-0.9320</td>\n",
              "      <td>-0.9200</td>\n",
              "      <td>-0.9424</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-0.932</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.985</td>\n",
              "      <td>-0.9463</td>\n",
              "      <td>-0.4020</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.3160</td>\n",
              "      <td>-0.0948</td>\n",
              "      <td>-0.4695</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>-0.9976</td>\n",
              "      <td>-0.9680</td>\n",
              "      <td>-1.0340</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.6980</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.4863</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>-0.3293</td>\n",
              "      <td>-0.0127</td>\n",
              "      <td>-0.1399</td>\n",
              "      <td>0.4624</td>\n",
              "      <td>-0.7610</td>\n",
              "      <td>-0.8696</td>\n",
              "      <td>0.1720</td>\n",
              "      <td>-0.0272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500000 rows √ó 562 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-260aeff3-5d3e-4432-8032-05ae93ea4164')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-260aeff3-5d3e-4432-8032-05ae93ea4164 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-260aeff3-5d3e-4432-8032-05ae93ea4164');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ef91c1b2-ea44-45eb-827c-ebf084e52c91\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef91c1b2-ea44-45eb-827c-ebf084e52c91')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ef91c1b2-ea44-45eb-827c-ebf084e52c91 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tYX"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tYX.y.value_counts(sort=False).to_frame().T  # counts of observations in each label category"
      ],
      "metadata": {
        "id": "5ip8I5XHzYfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmr = Timer() # runtime limit (in seconds). Add all of your code after the timer"
      ],
      "metadata": {
        "id": "1l4r_NEyzZ_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>‚è≥</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "Students: Keep all your definitions, code, documentation **between** ‚è≥ symbols."
      ],
      "metadata": {
        "id": "Vl15K7562P-n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "## **Task 1. Preprocessing Pipeline**\n",
        "\n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.\n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...?\n",
        "We put together our model by first looking at the data and learning from our past projects. The use of layers like batch normalization and dropout was based on their known benefits: batch normalization helps in stabilizing the learning process by normalizing the input to each activation function, and dropout prevents the model from relying too much on any single neuron, reducing overfitting. The decision to include these components was to ensure our model learns effectively from the training data and performs well on new, unseen data.\n",
        "\n",
        "\n",
        " Btw, EDA is not required)\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "\n",
        "To check if our model is on the right track, we watch its performance closely, focusing on accuracy and how many mistakes it makes during training and testing phases. This monitoring helps us see if the model is improving and if it can correctly handle data it hasn't seen before. By adjusting things like the dropout rate or the number of neurons in layers, we fine-tune our model for better results.\n",
        "\n",
        "\n",
        "1. What else have you tried that worked or didn't?\n",
        "\n",
        "In our journey to improve the model, we've experimented with adjusting the complexity of the model by adding or removing layers and changing their sizes. While simplifying the model's architecture or introducing ways to group data have sometimes sped up training and offered new insights, these adjustments haven't always led to better performance. Our experience has shown that the effectiveness of such strategies can vary significantly depending on the specifics of the data and the task at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30xYIFXAnaPE"
      },
      "source": [
        "**Student's answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "\n",
        "We designed our model with layers, batch normalization, and dropout, aiming to create a stable and generalizable system. This approach was shaped by our understanding of how each component contributes to overall model performance.\n",
        "\n",
        "\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "\n",
        "We used accuracy, loss metrics, and validation sets to measure our model's performance, ensuring it learns effectively and generalizes well to new data.\n",
        "\n",
        "\n",
        "1. What else have you tried that worked or didn't?\n",
        "\n",
        "Experimentation with various network architectures and regularization methods revealed the delicate balance between complexity and generalization, guiding us to optimize our model for the task at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "**Student's answer:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tYX.shape\n",
        "# tYX.head(10)\n",
        "# tYX.info()\n",
        "# tYX.describe()"
      ],
      "metadata": {
        "id": "53Ji5DHDhykQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tX, tY = tYX.drop('y', axis=1).head(50000), tYX.head(50000).y-1   # shift labels by -1 to range {0,1,2,3,4,5}"
      ],
      "metadata": {
        "id": "cSKHErv_xQKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, dropout_prob=0.1):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)  # Batch Normalization after the first layer\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # One Dropout layer before the final layer\n",
        "\n",
        "        self.fc5 = nn.Linear(32, 6)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))  # Applying Batch Normalization after the first layer\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "\n",
        "        x = self.dropout(x)  # Applying Dropout before the final layer\n",
        "\n",
        "        x = self.fc5(x)  # No activation here, assuming you'll use CrossEntropyLoss\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "m1Z3IvugwsAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy arrays to torch tensors\n",
        "tX_tensor = torch.tensor(tX.values, dtype=torch.float32)\n",
        "tY_tensor = torch.tensor(tY.values, dtype=torch.long)\n",
        "\n",
        "# If using GPU\n",
        "tX_tensor = tX_tensor.to(device)\n",
        "tY_tensor = tY_tensor.to(device)\n",
        "\n",
        "# Create TensorDataset and split into train val sets\n",
        "dataset = TensorDataset(tX_tensor, tY_tensor)\n",
        "val_size = int(len(dataset) * 0.3)\n",
        "train_size = len(dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Instantiate the model\n",
        "model = Model(input_size=tX.shape[1]).to(device)\n",
        "# print(summary(model, input_size=(1, tX.shape[1])))\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "# Training loop\n",
        "epochs = 16\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    # train_loss = running_loss / len(train_loader)\n",
        "    # train_acc = 100 * correct_train / total_train\n",
        "    # train_losses.append(train_loss)\n",
        "    # train_accuracies.append(train_acc)\n",
        "    # print(f\"Epoch {epoch+1}/{epochs}, Loss: {np.round(train_loss, 4)}, Accuracy: {np.round(train_acc, 4)}%\")\n",
        "\n",
        "    # # Validation Phase\n",
        "    # model.eval()\n",
        "    # val_loss = 0.0\n",
        "    # correct_val = 0\n",
        "    # total_val = 0\n",
        "    # with torch.no_grad():\n",
        "    #     for inputs, labels in val_loader:\n",
        "    #         outputs = model(inputs)\n",
        "    #         loss = criterion(outputs, labels)\n",
        "    #         val_loss += loss.item()\n",
        "\n",
        "    #         _, predicted = torch.max(outputs.data, 1)\n",
        "    #         total_val += labels.size(0)\n",
        "    #         correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    # val_loss = val_loss / len(val_loader)\n",
        "    # val_acc = 100 * correct_val / total_val\n",
        "    # val_losses.append(val_loss)\n",
        "    # val_accuracies.append(val_acc)\n",
        "    # print(f\"Validation Loss: {np.round(val_loss, 4)}, Accuracy: {np.round(val_acc, 4)}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67E2C2GzaRFf",
        "outputId": "df7facb9-09e0-4be2-b666-5302b6972cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=561, out_features=256, bias=True)\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc5): Linear(in_features=32, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# # Plotting\n",
        "# plt.figure(figsize=(12, 5))\n",
        "\n",
        "# # Loss Plot\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(train_losses, label='Training Loss')\n",
        "# plt.plot(val_losses, label='Validation Loss')\n",
        "# plt.title('Training and Validation Loss')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "\n",
        "# # Accuracy Plot\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(train_accuracies, label='Training Accuracy')\n",
        "# plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "# plt.title('Training and Validation Accuracy')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Ategr6frrlLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming vX is a numpy array containing your validation data\n",
        "# You need to convert it into a torch tensor with the correct type\n",
        "vX_tensor = torch.tensor(vX.values, dtype=torch.float32)\n",
        "\n",
        "# If using GPU, send the model and the tensor to GPU\n",
        "model.to('cuda')\n",
        "vX_tensor = vX_tensor.to('cuda')\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# No need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    # Make predictions\n",
        "    predictions = model(vX_tensor)\n",
        "\n",
        "# If you need class probabilities, apply softmax\n",
        "probabilities = torch.softmax(predictions, dim=1)\n",
        "\n",
        "# To get the predicted class labels, get the index of the max log-probability\n",
        "predicted_labels = torch.max(probabilities, 1)[1]\n",
        "\n",
        "# Convert to numpy array if needed (for further processing in non-PyTorch code)\n",
        "probabilities_np = probabilities.cpu().numpy()\n",
        "predicted_labels_np = predicted_labels.cpu().numpy()\n",
        "\n",
        "# Now 'probabilities_np' holds class probabilities and 'predicted_labels_np' holds class predictions"
      ],
      "metadata": {
        "id": "T8jgSQiPrBw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YLab = [f'{i}/{s}' for i, s in enumerate('walking walking_upstairs walking_downstairs sitting standing laying'.split())]  # column labels\n",
        "pd.DataFrame(probabilities_np[:3,:], columns=YLab).style.background_gradient(cmap='coolwarm', axis=1)  # display first few predictions"
      ],
      "metadata": {
        "id": "_kDPxmFqaRNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.DataFrame(predicted_labels_np + 1, columns=['y']) # labels are shifted to the initial state"
      ],
      "metadata": {
        "id": "JG6rFvOyir0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.T"
      ],
      "metadata": {
        "id": "hzGvrClxkB5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ToCSV(result, 'HAR_baseline') # generate a CSV submission file for Kaggle"
      ],
      "metadata": {
        "id": "jXh8V5NbaRTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References:**"
      ],
      "metadata": {
        "id": "pzBsjCvS_kEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ISL https://www.statlearning.com/\n",
        "2. ESL https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf\n",
        "3. D2L https://d2l.ai/"
      ],
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5>‚åõ</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ],
      "metadata": {
        "id": "DoF2GoB_QGw9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nosV1OWFJPx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a5f170-e6b3-4147-9798-c36ce6b156c7"
      },
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 52 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udpl5HJ4JSLr"
      },
      "source": [
        "# üí°**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Try tuning DNN hyperparameters\n",
        "1. Training set has 500K observations (2GB), but you really don't need them all. They are all bootstrapped (with noise) from the original sample of 7352 observations. In order to stay within Colab runtime limit (CRTL), you can\n",
        "  1. use more observations for a shallow DNN, but risk underfitting due to lower model complexity\n",
        "  1. use fewer observations for a deeper DNN, but risk overfitting to higher model complexity\n",
        "1. Check out the original related papers about feature engineering for this dataset\n",
        "1. Try engineering features with [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) and discarding unimportant features via PCA or alternative technique.\n",
        "1. Consider KMeans/KMedoid or other clustering methods to identify observations, which represent the original 7352 observations. It might require finding 7352 cluster centroids/medoids.\n",
        "  1. Fast clustering methods: [FAISS](https://github.com/facebookresearch/faiss) (GPU-enabled)\n",
        "1. For deep NN, consider dropout, batch normalization\n",
        "1. Try PCA on transposed matrix to find/eliminate highly correlated observations\n",
        "1. Try [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) to ensure each label is proportionally represented in a subsample"
      ],
      "metadata": {
        "id": "_KOCCeay2r6G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZNCqUiBhiQP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}